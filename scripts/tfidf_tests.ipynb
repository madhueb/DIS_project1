{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import sklearn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "import autofaiss\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available:  True\n",
      "GPU:  Quadro RTX 4000\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "print(\"Cuda is available: \", is_cuda)\n",
    "print(\"GPU: \", torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\" if is_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv('./Data/dev.csv')\n",
    "test = pd.read_csv('./Data/test.csv')\n",
    "train = pd.read_csv('./Data/train.csv')\n",
    "sample_submission = pd.read_csv('./Data/sample_submission.csv')\n",
    "with open(\"./Data/corpus.json\", \"r\") as f:\n",
    "    documents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_fr =[doc['text'] for doc in documents if doc['lang'] == 'fr']\n",
    "docs_fr_ids = [doc['docid'] for doc in documents if doc['lang'] == 'fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/eleves-b/2021/madeleine.hueber/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords_fr = set(stopwords.words('french'))\n",
    "stopwords_fr =list(stopwords_fr)\n",
    "def clean_text(text):\n",
    "    # soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    # for img in soup.find_all(\"img\"):\n",
    "    #     img.decompose()\n",
    "    #\n",
    "    # for table in soup.find_all(\"table\"):\n",
    "    #     table.decompose()\n",
    "\n",
    "    # clean_text = soup.get_text()\n",
    "    # Step 1: Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+|www\\.\\S+\", \" \", text)\n",
    "\n",
    "    # Step 2: Remove long sequences of non-alphanumeric characters (e.g., encoded data or code)\n",
    "    text = re.sub(r\"[^\\w\\s]{10,}\", \" \", text)  # Removes any sequence of 10 or more non-alphanumeric characters\n",
    "\n",
    "    # Step 3: Remove excessive whitespace\n",
    "    clean_text = re.sub(r\"\\s+\", \" \", text, flags=re.UNICODE).strip()\n",
    "\n",
    "    return clean_text\n",
    "def preprocess_text (text):\n",
    "    text = \" \".join([ent.text for ent in text.ents if ent.text not in stopwords_fr])\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10676/10676 [00:21<00:00, 504.74it/s]\n"
     ]
    }
   ],
   "source": [
    "test_docs = docs_fr[:5000]\n",
    "test_docs = [clean_text(doc) for doc in test_docs]\n",
    "docs_fr = [clean_text(doc) for doc in tqdm(docs_fr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:32<00:00, 10.85it/s]\n",
      "100%|██████████| 1000/1000 [01:35<00:00, 10.52it/s]\n",
      "100%|██████████| 1000/1000 [01:37<00:00, 10.31it/s]\n",
      "100%|██████████| 1000/1000 [06:07<00:00,  2.72it/s]\n",
      "100%|██████████| 1000/1000 [06:22<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "test_docs = [preprocess_text(doc) for doc in tqdm(nlp.pipe(test_docs,disable=[\"tagger\",\"parser\",\"textcat\"]),total=len(test_docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', max_df = 0.1,stop_words=stopwords_fr)\n",
    "doc_vectors_2 = vectorizer.fit_transform(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 278006)\n"
     ]
    }
   ],
   "source": [
    "print(doc_vectors_2.shape)\n",
    "doc_vectors_2 = doc_vectors_2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 12:16:49,857 [INFO]: Using 16 omp threads (processes), consider increasing --nb_cores if you have more\n",
      "2024-11-02 12:16:52,615 [INFO]: Launching the whole pipeline 11/02/2024, 12:16:52\n",
      "2024-11-02 12:16:52,615 [INFO]: Reading total number of vectors and dimension 11/02/2024, 12:16:52\n",
      "100%|██████████| 1/1 [00:00<00:00, 24672.38it/s]\n",
      "2024-11-02 12:16:52,633 [INFO]: There are 5000 embeddings of dim 278006\n",
      "2024-11-02 12:16:52,634 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.0181 secs\n",
      "2024-11-02 12:16:52,634 [INFO]: \tCompute estimated construction time of the index 11/02/2024, 12:16:52\n",
      "2024-11-02 12:16:52,634 [INFO]: \t\t-> Train: 16.7 minutes\n",
      "2024-11-02 12:16:52,635 [INFO]: \t\t-> Add: 15.5 seconds\n",
      "2024-11-02 12:16:52,635 [INFO]: \t\tTotal: 16.9 minutes\n",
      "2024-11-02 12:16:52,635 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0013 secs\n",
      "2024-11-02 12:16:52,636 [INFO]: \tChecking that your have enough memory available to create the index 11/02/2024, 12:16:52\n",
      "2024-11-02 12:16:52,636 [INFO]: 5.7GB of memory will be needed to build the index (more might be used if you have more)\n",
      "2024-11-02 12:16:52,637 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0008 secs\n",
      "2024-11-02 12:16:52,637 [INFO]: \tSelecting most promising index types given data characteristics 11/02/2024, 12:16:52\n",
      "2024-11-02 12:16:52,637 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n",
      "2024-11-02 12:16:52,638 [INFO]: \tCreating the index 11/02/2024, 12:16:52\n",
      "2024-11-02 12:16:52,638 [INFO]: \t\t-> Instanciate the index HNSW15 11/02/2024, 12:16:52\n",
      "2024-11-02 12:16:52,638 [INFO]: \t\t>>> Finished \"-> Instanciate the index HNSW15\" in 0.0001 secs\n",
      "2024-11-02 12:16:52,639 [INFO]: \t\t-> Adding the vectors to the index 11/02/2024, 12:16:52\n",
      "2024-11-02 12:16:52,640 [INFO]: The memory available for adding the vectors is 26.8GB(total available - used by the index)\n",
      "2024-11-02 12:16:52,640 [INFO]: Using a batch size of 899 (memory overhead 953.4MB)\n",
      "100%|██████████| 228/228 [03:39<00:00,  1.04it/s]\n",
      "2024-11-02 12:20:31,676 [INFO]: \tComputing best hyperparameters for index my_index.faiss 11/02/2024, 12:20:31\n",
      "2024-11-02 12:23:37,516 [INFO]: \t>>> Finished \"Computing best hyperparameters for index my_index.faiss\" in 185.8392 secs\n",
      "2024-11-02 12:23:37,516 [INFO]: The best hyperparameters are: efSearch=16\n",
      "2024-11-02 12:23:37,517 [INFO]: \tCompute fast metrics 11/02/2024, 12:23:37\n",
      " 98%|█████████▊| 45/46 [00:00<00:00, 109.67it/s]\n",
      "2024-11-02 12:23:49,820 [INFO]: \t>>> Finished \"Compute fast metrics\" in 12.3025 secs\n",
      "2024-11-02 12:23:49,821 [INFO]: \tSaving the index on local disk 11/02/2024, 12:23:49\n",
      "2024-11-02 12:24:41,381 [INFO]: \t>>> Finished \"Saving the index on local disk\" in 51.5599 secs\n",
      "2024-11-02 12:24:41,382 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 468.7424 secs\n",
      "2024-11-02 12:24:41,383 [INFO]: {\n",
      "2024-11-02 12:24:41,383 [INFO]: \tindex_key: HNSW15\n",
      "2024-11-02 12:24:41,383 [INFO]: \tindex_param: efSearch=16\n",
      "2024-11-02 12:24:41,384 [INFO]: \tindex_path: my_index.faiss\n",
      "2024-11-02 12:24:41,384 [INFO]: \tsize in bytes: 5560800230\n",
      "2024-11-02 12:24:41,384 [INFO]: \tavg_search_speed_ms: 15.701945782295505\n",
      "2024-11-02 12:24:41,384 [INFO]: \t99p_search_speed_ms: 24.652213687077158\n",
      "2024-11-02 12:24:41,385 [INFO]: \treconstruction error %: 0.0\n",
      "2024-11-02 12:24:41,385 [INFO]: \tnb vectors: 5000\n",
      "2024-11-02 12:24:41,385 [INFO]: \tvectors dimension: 278006\n",
      "2024-11-02 12:24:41,385 [INFO]: \tcompression ratio: 0.9998776740807321\n",
      "2024-11-02 12:24:41,386 [INFO]: }\n",
      "2024-11-02 12:24:41,387 [INFO]: \t>>> Finished \"Creating the index\" in 468.7485 secs\n",
      "2024-11-02 12:24:41,387 [INFO]: >>> Finished \"Launching the whole pipeline\" in 468.7719 secs\n"
     ]
    }
   ],
   "source": [
    "# load document embeddings\n",
    "d = doc_vectors_2.shape[1] # dimension of the embeddings\n",
    "N = 30 # nb of chunks neighbors in the graph\n",
    "\n",
    "#Normalize vectors \n",
    "doc_vectors_2 = doc_vectors_2 / np.linalg.norm(doc_vectors_2, axis=1)[:, None]\n",
    "\n",
    "#Create Index \n",
    "index = autofaiss.build_index(doc_vectors_2, save_on_disk=True, index_path=\"my_index.faiss\", max_index_memory_usage=\"2GB\",metric_type=faiss.METRIC_INNER_PRODUCT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"my_index.faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id                                                   q-fr-43\n",
      "query            identiques, mais il y a une légère différence ...\n",
      "positive_docs                                           doc-fr-344\n",
      "negative_docs    ['doc-fr-345', 'doc-fr-346', 'doc-fr-347', 'do...\n",
      "lang                                                            fr\n",
      "Name: 206, dtype: object\n",
      "2360\n"
     ]
    }
   ],
   "source": [
    "i=5\n",
    "print(dev[dev[\"lang\"] == \"fr\"].iloc[i])\n",
    "query = dev[dev[\"lang\"] == \"fr\"].iloc[i][\"query\"] \n",
    "\n",
    "idx = dev[dev[\"lang\"] == \"fr\"].iloc[i][\"positive_docs\"]\n",
    "idx = docs_fr_ids.index(idx)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1815349/4013051727.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  query = query / np.linalg.norm(query, axis=1)[:, None]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = clean_text(query)\n",
    "if len(preprocess_text(nlp(query))) >0 :\n",
    "    query = preprocess_text(nlp(query))\n",
    "query = vectorizer.transform([query])\n",
    "query = query.toarray()\n",
    "query = query / np.linalg.norm(query, axis=1)[:, None]\n",
    "\n",
    "print(query@doc_vectors_2[idx])\n",
    "\n",
    "distances, indices = index.search(query, 5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(docs_fr_ids[indices[0][i]])\n",
    "    print(docs_fr[indices[0][i]])\n",
    "    print(distances[0][i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
