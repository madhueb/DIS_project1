{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'fr'\n",
    "\n",
    "with open(f\"./tokens_merged/tokens_{lang}.pkl\", mode=\"rb\") as f:\n",
    "    ids, corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"./data/train.csv\")\n",
    "dev_df = pd.read_csv(f\"./data/dev.csv\")\n",
    "test_df = pd.read_csv(f\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df[\"lang\"] == lang]\n",
    "dev_df = dev_df[dev_df[\"lang\"] == lang]\n",
    "test_df = test_df[test_df[\"lang\"] == lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"positive_docs_i\"] = train_df[\"positive_docs\"].apply(lambda x: ids.index(x))\n",
    "dev_df[\"positive_docs_i\"] = dev_df[\"positive_docs\"].apply(lambda x: ids.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>positive_docs</th>\n",
       "      <th>negative_docs</th>\n",
       "      <th>lang</th>\n",
       "      <th>positive_docs_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>q-fr-1080</td>\n",
       "      <td>Quand Antoine Meillet est-il né ?</td>\n",
       "      <td>doc-fr-7715</td>\n",
       "      <td>['doc-fr-4657', 'doc-fr-2635', 'doc-fr-7352', ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>3066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>q-fr-1081</td>\n",
       "      <td>Quelles sont les origines de l'algèbre linéair...</td>\n",
       "      <td>doc-fr-7723</td>\n",
       "      <td>['doc-fr-1298', 'doc-fr-4506', 'doc-fr-6921', ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>3074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>q-fr-1082</td>\n",
       "      <td>Quelle est l'étymologie du mot \"algorithme\" et...</td>\n",
       "      <td>doc-fr-7731</td>\n",
       "      <td>['doc-fr-3025', 'doc-fr-3923', 'doc-fr-5672', ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>3082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>q-fr-1083</td>\n",
       "      <td>Quels sont les pouvoirs exécutif, législatif e...</td>\n",
       "      <td>doc-fr-7739</td>\n",
       "      <td>['doc-fr-840', 'doc-fr-7178', 'doc-fr-2238', '...</td>\n",
       "      <td>fr</td>\n",
       "      <td>3090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>q-fr-1084</td>\n",
       "      <td>Quelle est la langue officielle de l'Autriche ?</td>\n",
       "      <td>doc-fr-7747</td>\n",
       "      <td>['doc-fr-2144', 'doc-fr-5969', 'doc-fr-3666', ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>3098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        query_id                                              query  \\\n",
       "10000  q-fr-1080                  Quand Antoine Meillet est-il né ?   \n",
       "10001  q-fr-1081  Quelles sont les origines de l'algèbre linéair...   \n",
       "10002  q-fr-1082  Quelle est l'étymologie du mot \"algorithme\" et...   \n",
       "10003  q-fr-1083  Quels sont les pouvoirs exécutif, législatif e...   \n",
       "10004  q-fr-1084    Quelle est la langue officielle de l'Autriche ?   \n",
       "\n",
       "      positive_docs                                      negative_docs lang  \\\n",
       "10000   doc-fr-7715  ['doc-fr-4657', 'doc-fr-2635', 'doc-fr-7352', ...   fr   \n",
       "10001   doc-fr-7723  ['doc-fr-1298', 'doc-fr-4506', 'doc-fr-6921', ...   fr   \n",
       "10002   doc-fr-7731  ['doc-fr-3025', 'doc-fr-3923', 'doc-fr-5672', ...   fr   \n",
       "10003   doc-fr-7739  ['doc-fr-840', 'doc-fr-7178', 'doc-fr-2238', '...   fr   \n",
       "10004   doc-fr-7747  ['doc-fr-2144', 'doc-fr-5969', 'doc-fr-3666', ...   fr   \n",
       "\n",
       "       positive_docs_i  \n",
       "10000             3066  \n",
       "10001             3074  \n",
       "10002             3082  \n",
       "10003             3090  \n",
       "10004             3098  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>positive_docs</th>\n",
       "      <th>negative_docs</th>\n",
       "      <th>lang</th>\n",
       "      <th>positive_docs_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>q-fr-0</td>\n",
       "      <td>Quels sont les chiffres concernant les violenc...</td>\n",
       "      <td>doc-fr-0</td>\n",
       "      <td>['doc-fr-1', 'doc-fr-2', 'doc-fr-3', 'doc-fr-4...</td>\n",
       "      <td>fr</td>\n",
       "      <td>2710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>q-fr-1</td>\n",
       "      <td>complex au fil du temps. Quelle est la contrib...</td>\n",
       "      <td>doc-fr-8</td>\n",
       "      <td>['doc-fr-9', 'doc-fr-10', 'doc-fr-11', 'doc-fr...</td>\n",
       "      <td>fr</td>\n",
       "      <td>1244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>q-fr-6</td>\n",
       "      <td>Quel est le projet de Fiorini dans ses Gravure...</td>\n",
       "      <td>doc-fr-48</td>\n",
       "      <td>['doc-fr-49', 'doc-fr-50', 'doc-fr-51', 'doc-f...</td>\n",
       "      <td>fr</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>q-fr-19</td>\n",
       "      <td>Quelles étaient les conséquences de l'assassin...</td>\n",
       "      <td>doc-fr-152</td>\n",
       "      <td>['doc-fr-153', 'doc-fr-154', 'doc-fr-155', 'do...</td>\n",
       "      <td>fr</td>\n",
       "      <td>2922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>q-fr-34</td>\n",
       "      <td>omment Cochrane réussit-il à tromper la frégat...</td>\n",
       "      <td>doc-fr-272</td>\n",
       "      <td>['doc-fr-273', 'doc-fr-274', 'doc-fr-275', 'do...</td>\n",
       "      <td>fr</td>\n",
       "      <td>1546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    query_id                                              query positive_docs  \\\n",
       "200   q-fr-0  Quels sont les chiffres concernant les violenc...      doc-fr-0   \n",
       "201   q-fr-1  complex au fil du temps. Quelle est la contrib...      doc-fr-8   \n",
       "202   q-fr-6  Quel est le projet de Fiorini dans ses Gravure...     doc-fr-48   \n",
       "203  q-fr-19  Quelles étaient les conséquences de l'assassin...    doc-fr-152   \n",
       "204  q-fr-34  omment Cochrane réussit-il à tromper la frégat...    doc-fr-272   \n",
       "\n",
       "                                         negative_docs lang  positive_docs_i  \n",
       "200  ['doc-fr-1', 'doc-fr-2', 'doc-fr-3', 'doc-fr-4...   fr             2710  \n",
       "201  ['doc-fr-9', 'doc-fr-10', 'doc-fr-11', 'doc-fr...   fr             1244  \n",
       "202  ['doc-fr-49', 'doc-fr-50', 'doc-fr-51', 'doc-f...   fr              219  \n",
       "203  ['doc-fr-153', 'doc-fr-154', 'doc-fr-155', 'do...   fr             2922  \n",
       "204  ['doc-fr-273', 'doc-fr-274', 'doc-fr-275', 'do...   fr             1546  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTokenizer:\n",
    "    def __init__(self, model_name: str):\n",
    "        spacy.cli.download(model_name)\n",
    "        self.nlp = spacy.load(model_name, exclude=[\"senter\", \"ner\"])\n",
    "        self.stop_words = set(self.nlp.Defaults.stop_words)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text: str) -> str:\n",
    "        text = re.sub(r\"http[s]?://\\S+|www\\.\\S+\", \" \", text)\n",
    "        text = re.sub(r\"[^\\w\\s]{4,}\", \" \", text)\n",
    "        return re.sub(r\"\\s+\", \" \", text.replace(\"\\n\", \" \")).strip().lower()\n",
    "\n",
    "    def tokenize_batch(\n",
    "        self, texts: List[str], batch_size: int = 64, n_process: int = 8\n",
    "    ):\n",
    "        print(\"Tokenizing...\")\n",
    "        preprocessed_texts = [self.preprocess_text(text) for text in texts]\n",
    "        print(\"Preprocessed...\")\n",
    "        docs = self.nlp.pipe(\n",
    "            preprocessed_texts, batch_size=batch_size, n_process=n_process\n",
    "        )\n",
    "        print(\"Docs...\")\n",
    "        tokenized_texts = [\n",
    "            [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "            for doc in tqdm(docs, total=len(preprocessed_texts))\n",
    "        ]\n",
    "        return tokenized_texts\n",
    "\n",
    "\n",
    "class EnglishTokenizer(BaseTokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "class FrenchTokenizer(BaseTokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"fr_core_news_sm\")\n",
    "\n",
    "\n",
    "class GermanTokenizer(BaseTokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"de_core_news_sm\")\n",
    "\n",
    "\n",
    "class ItalianTokenizer(BaseTokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"it_core_news_sm\")\n",
    "\n",
    "\n",
    "class SpanishTokenizer(BaseTokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"es_core_news_sm\")\n",
    "\n",
    "\n",
    "class KoreanTokenizer(BaseTokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"ko_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "if lang == \"en\":\n",
    "    tokenizer = EnglishTokenizer()\n",
    "elif lang == \"fr\":\n",
    "    tokenizer = FrenchTokenizer()\n",
    "elif lang == \"de\":\n",
    "    tokenizer = GermanTokenizer()\n",
    "elif lang == \"it\":\n",
    "    tokenizer = ItalianTokenizer()\n",
    "elif lang == \"es\":\n",
    "    tokenizer = SpanishTokenizer()\n",
    "elif lang == \"ko\":\n",
    "    tokenizer = KoreanTokenizer()\n",
    "else:\n",
    "    raise KeyError(\"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Preprocessed...\n",
      "Docs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1608/1608 [00:03<00:00, 505.07it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_queries = tokenizer.tokenize_batch(train_df[\"query\"].tolist(), batch_size=32, n_process=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10676/10676 [00:02<00:00, 3997.62it/s]\n",
      "100%|██████████| 10676/10676 [00:04<00:00, 2494.82it/s]\n",
      "100%|██████████| 12944661/12944661 [00:22<00:00, 575456.33it/s]\n",
      "100%|██████████| 1608/1608 [00:01<00:00, 862.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42039800995024873"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.bm25.bm25 import BM25\n",
    "\n",
    "bm25 = BM25()\n",
    "bm25.fit(corpus)\n",
    "\n",
    "ranks = []\n",
    "for i, tokenized_query in enumerate(tqdm(tokenized_queries, total=len(tokenized_queries))):\n",
    "    target = train_df.iloc[i][\"positive_docs_i\"]\n",
    "    scores = bm25._scores(tokenized_query)\n",
    "    score_target = scores[target]\n",
    "    rank_target = sum(score_target <= score for score in scores)\n",
    "    ranks.append(rank_target)\n",
    "\n",
    "# recall @ 10\n",
    "recall_10 = sum(rank <= 10 for rank in ranks) / len(ranks)\n",
    "recall_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "100%|██████████| 1608/1608 [01:45<00:00, 15.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34577114427860695"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # use tf-idf\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     tokenizer=lambda x: x, preprocessor=lambda x: x, lowercase=False\n",
    "# )\n",
    "# X = vectorizer.fit_transform(corpus)\n",
    "# X\n",
    "\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ranks_tfidf = []\n",
    "\n",
    "# for i, tokenized_query in enumerate(tqdm(tokenized_queries, total=len(tokenized_queries))):\n",
    "#     target = train_df.iloc[i][\"positive_docs_i\"]\n",
    "#     query_vector = vectorizer.transform([tokenized_query])\n",
    "#     scores = cosine_similarity(X, query_vector).ravel()\n",
    "#     score_target = scores[target]\n",
    "#     rank_target = sum(score_target <= score for score in scores)\n",
    "#     ranks_tfidf.append(rank_target)\n",
    "\n",
    "# recall_10_tfidf = sum(rank <= 10 for rank in ranks_tfidf) / len(ranks_tfidf)\n",
    "# recall_10_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1608/1608 [00:52<00:00, 30.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42039800995024873"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from rank_bm25 import BM25Plus\n",
    "\n",
    "# bm25_plus = BM25Plus(corpus)\n",
    "\n",
    "# ranks_bm25_plus = []\n",
    "\n",
    "# for i, tokenized_query in enumerate(tqdm(tokenized_queries, total=len(tokenized_queries))):\n",
    "#     target = train_df.iloc[i][\"positive_docs_i\"]\n",
    "#     scores = bm25_plus.get_scores(tokenized_query)\n",
    "#     score_target = scores[target]\n",
    "#     rank_target = sum(score_target <= score for score in scores)\n",
    "#     ranks_bm25_plus.append(rank_target)\n",
    "\n",
    "# recall_10_bm25_plus = sum(rank <= 10 for rank in ranks_bm25_plus) / len(ranks_bm25_plus)\n",
    "# recall_10_bm25_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "# import numpy as np\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# target_indices = train_df[\"positive_docs_i\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings for corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1284/10676 [40:19<4:54:58,  1.88s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate embeddings for each document in the corpus, with progress tracking\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating embeddings for corpus...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m corpus_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[43mget_average_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(corpus)]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Function to perform retrieval for a query and return top-k results\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_top_k\u001b[39m(query_tokens, corpus_embeddings, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mget_average_embedding\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_average_embedding\u001b[39m(tokens):\n\u001b[0;32m---> 11\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m [\u001b[43mft_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_word_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m ft_model]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vectors:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(vectors, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fasttext/FastText.py:131\u001b[0m, in \u001b[0;36m_FastText.get_word_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    128\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mgetArgs()\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdim\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_word_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the vector representation of word.\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dimension()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Load the pre-trained FastText model for French\n",
    "# ft_model = fasttext.load_model('cc.fr.300.bin')  # Specify the path to the downloaded model\n",
    "\n",
    "# # Assuming `corpus`, `tokenized_queries`, and `target_indices` are defined\n",
    "# # `corpus`: List of tokenized documents\n",
    "# # `tokenized_queries`: List of tokenized query terms\n",
    "# # `target_indices`: List of indices in `corpus` that are the correct document for each query\n",
    "\n",
    "# # Function to get average FastText vector for a list of tokens\n",
    "# def get_average_embedding(tokens):\n",
    "#     vectors = [ft_model.get_word_vector(token) for token in tokens if token in ft_model]\n",
    "#     if vectors:\n",
    "#         return np.mean(vectors, axis=0)\n",
    "#     else:\n",
    "#         return np.zeros(ft_model.get_dimension())\n",
    "\n",
    "# # Calculate embeddings for each document in the corpus, with progress tracking\n",
    "# print(\"Calculating embeddings for corpus...\")\n",
    "# corpus_embeddings = [get_average_embedding(doc) for doc in tqdm(corpus)]\n",
    "\n",
    "# # Function to perform retrieval for a query and return top-k results\n",
    "# def retrieve_top_k(query_tokens, corpus_embeddings, k=10):\n",
    "#     query_embedding = get_average_embedding(query_tokens)\n",
    "#     similarities = cosine_similarity([query_embedding], corpus_embeddings).flatten()\n",
    "#     top_k_indices = np.argsort(similarities)[-k:][::-1]  # Indices of top-k most similar documents\n",
    "#     return top_k_indices\n",
    "\n",
    "# # Calculate recall@10\n",
    "# correct_retrievals = 0\n",
    "# k = 10  # Setting k=10 for recall@10\n",
    "\n",
    "# print(\"Calculating recall@10 for each query...\")\n",
    "# for i, q_tokens in enumerate(tqdm(tokenized_queries)):\n",
    "#     top_k_indices = retrieve_top_k(q_tokens, corpus_embeddings, k)\n",
    "#     if target_indices[i] in top_k_indices:\n",
    "#         correct_retrievals += 1\n",
    "\n",
    "# # Calculate recall@10 as a percentage\n",
    "# recall_at_10 = correct_retrievals / len(tokenized_queries) * 100\n",
    "# print(f\"Recall@10: {recall_at_10:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
